{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM3bblYaDIFrYQV49QHiAk5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilayRaut/Self-Alignment-with-Instruction-Backtranslation/blob/main/Self_Alignment_with_Instruction_Backtranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Alignment with Instruction Backtranslation\n",
        "\n",
        "Nilay Raut\n",
        "\n",
        "This notebook implements the paper: https://arxiv.org/pdf/2308.06259.pdf\n",
        "\n",
        "\n",
        "\n",
        "##1: Installation (Run this first)"
      ],
      "metadata": {
        "id": "d2AM7HsTIT_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "# !pip install -q datasets==2.14.0\n",
        "# !pip install -q transformers==4.34.0 huggingface_hub==0.20.0\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q huggingface_hub"
      ],
      "metadata": {
        "id": "X5iDHE1uINU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2: Import Libraries\n"
      ],
      "metadata": {
        "id": "LqgGFyyvIfeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Set seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "d05WX3xORx9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3: Configuration and Setup"
      ],
      "metadata": {
        "id": "LHmL0BD1R0XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Update this with your HuggingFace username!\n",
        "HF_USERNAME = \"NilayR\"  # TODO: CHANGE THIS!\n",
        "\n",
        "# Configuration for optimization\n",
        "CONFIG = {\n",
        "    \"use_cpu_prototype\": False,  # Start with CPU\n",
        "    \"backward_dataset_size\": 3000,  # Small for prototyping\n",
        "    \"lima_sample_size\": 150,  # As required\n",
        "    \"max_steps\": 150,  # Reduced for faster training\n",
        "    \"batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"max_length\": 256,  # Reduced from 512\n",
        "    \"lora_r\": 8,  # Reduced from 16\n",
        "    \"lora_alpha\": 16,\n",
        "}\n",
        "\n",
        "print(f\"Configuration loaded. Models will be pushed to: {HF_USERNAME}\")"
      ],
      "metadata": {
        "id": "iUwroZvVR72c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Change to full training i.e when using gpu\n",
        "# CONFIG[\"use_cpu_prototype\"] = False\n",
        "# CONFIG[\"backward_dataset_size\"] = 3000  # Increase if needed\n",
        "# CONFIG[\"max_steps\"] = 150  # Can increase to 200-300"
      ],
      "metadata": {
        "id": "0R40tQpuNQ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4: HuggingFace Login"
      ],
      "metadata": {
        "id": "mrOg7kcqR80p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HuggingFace\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    hf_token = userdata.get('huggingface')\n",
        "    login(token=hf_token)\n",
        "    print(\"Logged in to HuggingFace\")\n",
        "except:\n",
        "    from getpass import getpass\n",
        "    hf_token = getpass(\"Enter HuggingFace token: \")\n",
        "    login(token=hf_token)\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)"
      ],
      "metadata": {
        "id": "6N9idmK6SA7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5: Check Device"
      ],
      "metadata": {
        "id": "YgPFFivaSC_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\" No GPU available. Using CPU for prototyping.\")"
      ],
      "metadata": {
        "id": "mKn0Uze7SGMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6: Load and Prepare Guanaco Dataset"
      ],
      "metadata": {
        "id": "sxUXudeFSHw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Guanaco manually\n",
        "import json\n",
        "import requests\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"\\n Loading OpenAssistant Guanaco dataset...\")\n",
        "\n",
        "url = \"https://huggingface.co/datasets/timdettmers/openassistant-guanaco/resolve/main/openassistant_best_replies_train.jsonl\"\n",
        "response = requests.get(url)\n",
        "\n",
        "data = []\n",
        "for line in response.text.strip().split('\\n'):\n",
        "    if line:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "dataset_guanaco = Dataset.from_list(data)\n",
        "print(f\"Total examples: {len(dataset_guanaco)}\")"
      ],
      "metadata": {
        "id": "PgSjh-HySpTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7: Extract Instruction-Output Pairs"
      ],
      "metadata": {
        "id": "knVvyG9ZSrZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pairs_from_guanaco(example):\n",
        "    \"\"\"Extract clean instruction-output pairs\"\"\"\n",
        "    text = example['text']\n",
        "    pairs = []\n",
        "\n",
        "    if '### Human:' in text and '### Assistant:' in text:\n",
        "        parts = text.split('### Human:')\n",
        "        for part in parts[1:]:\n",
        "            if '### Assistant:' in part:\n",
        "                human_assistant = part.split('### Assistant:')\n",
        "                if len(human_assistant) >= 2:\n",
        "                    instruction = human_assistant[0].strip()\n",
        "                    response = human_assistant[1].split('### Human:')[0].strip()\n",
        "                    # Filter by length\n",
        "                    if instruction and response and len(instruction) < 500 and len(response) < 800:\n",
        "                        pairs.append({\n",
        "                            'instruction': instruction,\n",
        "                            'output': response\n",
        "                        })\n",
        "    return pairs\n",
        "\n",
        "# Extract pairs (small subset for CPU)\n",
        "print(\"Extracting instruction-output pairs...\")\n",
        "all_pairs = []\n",
        "subset_size = 5000 if CONFIG[\"use_cpu_prototype\"] else len(dataset_guanaco)\n",
        "\n",
        "for i, example in enumerate(tqdm(dataset_guanaco.select(range(min(subset_size, len(dataset_guanaco)))), desc=\"Processing\")):\n",
        "    pairs = extract_pairs_from_guanaco(example)\n",
        "    all_pairs.extend(pairs)\n",
        "\n",
        "print(f\"Extracted {len(all_pairs)} pairs\")"
      ],
      "metadata": {
        "id": "xc45TD5hSuP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8: Create Backward Dataset"
      ],
      "metadata": {
        "id": "C5b-t-QmSxxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create backward training data (output -> instruction)\n",
        "backward_texts = []\n",
        "num_examples = min(CONFIG[\"backward_dataset_size\"], len(all_pairs))\n",
        "\n",
        "for i in range(num_examples):\n",
        "    pair = all_pairs[i]\n",
        "    # Backward format: given output, predict instruction\n",
        "    text = f\"### Output:\\n{pair['output']}\\n\\n### Instruction:\\n{pair['instruction']}\"\n",
        "    backward_texts.append(text)\n",
        "\n",
        "backward_dataset = Dataset.from_dict({'text': backward_texts})\n",
        "print(f\" Backward dataset created: {len(backward_dataset)} examples\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\n Example backward data:\")\n",
        "print(backward_dataset[0]['text'][:300] + \"...\")"
      ],
      "metadata": {
        "id": "KlIt907ZTE9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9: Setup Model and Tokenizer"
      ],
      "metadata": {
        "id": "o31trIKtTGm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Setting up model and tokenizer...\")\n",
        "\n",
        "# Model configuration\n",
        "base_model_id = \"NousResearch/Llama-2-7b-chat-hf\"  # Using chat version for faster convergence\n",
        "\n",
        "# Quantization config for GPU (will be None for CPU)\n",
        "if device.type == \"cuda\":\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "else:\n",
        "    quantization_config = None\n",
        "    print(\" CPU mode: No quantization\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "KIO-FKaiTKFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10: Tokenization Function"
      ],
      "metadata": {
        "id": "onmLjfA_TM4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=CONFIG[\"max_length\"],\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "# Tokenize backward dataset\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_backward = backward_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "tokenized_backward = tokenized_backward.map(\n",
        "    lambda examples: {\"labels\": examples[\"input_ids\"]},\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "tg-AYGYnTTA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11: Initialize Backward Model (CPU Prototype)"
      ],
      "metadata": {
        "id": "xhqtKJYFTVZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG[\"use_cpu_prototype\"]:\n",
        "    print(\"\\n CPU Prototype: Loading small model...\")\n",
        "    # For CPU, we'll use a tiny model for testing\n",
        "    prototype_model_id = \"gpt2\"  # Small model for CPU testing\n",
        "    backward_model = AutoModelForCausalLM.from_pretrained(prototype_model_id)\n",
        "    prototype_tokenizer = AutoTokenizer.from_pretrained(prototype_model_id)\n",
        "    prototype_tokenizer.pad_token = prototype_tokenizer.eos_token\n",
        "\n",
        "    print(\" Loaded small model for CPU prototyping\")\n",
        "else:\n",
        "    # Full model for GPU\n",
        "    print(\"\\n Loading Llama-2-7b model...\")\n",
        "    backward_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" if device.type == \"cuda\" else None,\n",
        "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
        "    )\n",
        "    if device.type == \"cuda\":\n",
        "        backward_model = prepare_model_for_kbit_training(backward_model)"
      ],
      "metadata": {
        "id": "tGzNfq4JTaTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12: Configure LoRA"
      ],
      "metadata": {
        "id": "dban28vNTb51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=CONFIG[\"lora_r\"],\n",
        "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
        "    target_modules=[\"q_proj\", \"v_proj\"] if not CONFIG[\"use_cpu_prototype\"] else [\"c_attn\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "if not CONFIG[\"use_cpu_prototype\"]:\n",
        "    backward_model = get_peft_model(backward_model, lora_config)\n",
        "    backward_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "_Q6Hin38Tg0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13: Setup Training Arguments"
      ],
      "metadata": {
        "id": "BS2AVnQiTizG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./backward_model\",\n",
        "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
        "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
        "    max_steps=CONFIG[\"max_steps\"],\n",
        "    learning_rate=3e-5,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"no\",  # Don't save checkpoints\n",
        "    fp16=device.type == \"cuda\",\n",
        "    optim=\"adamw_torch\" if CONFIG[\"use_cpu_prototype\"] else \"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "B0VM4NfiUDbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14: Train Backward Model\n",
        "\n",
        "This step trains the backward model (`backward_model`) on the prepared Guanaco dataset (`tokenized_backward`). The model is fine-tuned to learn the mapping from an output (`y`) back to its original instruction (`x`), effectively creating the $p(x|y)$ model required for instruction backtranslation. The training uses the configurations defined earlier, including LoRA and the specified training arguments."
      ],
      "metadata": {
        "id": "2Zo3lsOlUGdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüöÄ Training backward model...\")\n",
        "print(f\"Training on: {device}\")\n",
        "print(f\"Max steps: {CONFIG['max_steps']}\")\n",
        "\n",
        "# Create trainer\n",
        "trainer_backward = Trainer(\n",
        "    model=backward_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_backward,\n",
        "    tokenizer=prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(\n",
        "        tokenizer=prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer_backward.train()\n",
        "print(\"‚úÖ Backward model training complete!\")\n"
      ],
      "metadata": {
        "id": "0MdiXq11UHCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15: Save and Upload Backward Model\n",
        "\n",
        "After training, the backward model is saved locally and then pushed to the Hugging Face Hub. This makes the trained model available for later use in the self-augmentation phase and for sharing. The URL to the uploaded model will be printed below."
      ],
      "metadata": {
        "id": "x_5oWi7jUK9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not CONFIG[\"use_cpu_prototype\"]:\n",
        "    print(\"\\nüíæ Saving and uploading backward model...\")\n",
        "    backward_model_name = f\"{HF_USERNAME}/llama2-7b-backward-instruction\"\n",
        "\n",
        "    # Save locally first\n",
        "    trainer_backward.save_model(\"./models/backward_model\")\n",
        "    tokenizer.save_pretrained(\"./models/backward_model\")\n",
        "\n",
        "    # Push to hub\n",
        "    backward_model.push_to_hub(backward_model_name, use_auth_token=True)\n",
        "    tokenizer.push_to_hub(backward_model_name, use_auth_token=True)\n",
        "\n",
        "    backward_model_url = f\"https://huggingface.co/{backward_model_name}\"\n",
        "    print(f\"‚úÖ BACKWARD MODEL URL: {backward_model_url}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CPU prototype mode - skipping upload\")\n",
        "    backward_model_url = \"CPU_PROTOTYPE_MODE\""
      ],
      "metadata": {
        "id": "s1vDK2SQUYtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##16: Load LIMA Dataset"
      ],
      "metadata": {
        "id": "0UEHnPMlUbJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install compatible version of datasets\n",
        "# !pip install datasets==2.14.0 -q"
      ],
      "metadata": {
        "id": "REVYg6LXSDt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: Self-Augmentation with LIMA\")\n",
        "print(\"=\"*70)\n",
        "print(\"üìö Loading LIMA dataset...\")\n",
        "\n",
        "# Method 1: Use huggingface_hub to download properly\n",
        "try:\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    filepath = hf_hub_download(\n",
        "        repo_id=\"GAIR/lima\",\n",
        "        filename=\"train.jsonl\",\n",
        "        repo_type=\"dataset\"\n",
        "    )\n",
        "\n",
        "    import json\n",
        "    data = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "\n",
        "    dataset_lima = Dataset.from_list(data)\n",
        "    print(f\"‚úÖ Loaded LIMA dataset: {len(dataset_lima)} examples\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LIMA dataset: {e}\")\n",
        "    print(\"Failed to load LIMA dataset using hf_hub_download.\")\n",
        "    # Handle error appropriately, e.g., initialize as empty\n",
        "    dataset_lima = []\n",
        "\n",
        "\n",
        "if 'dataset_lima' in locals() and dataset_lima:\n",
        "    print(f\"Total LIMA examples: {len(dataset_lima)}\")\n",
        "else:\n",
        "    print(\"Failed to load LIMA dataset or it's empty.\")\n",
        "    # You might want to add code here to exit or skip subsequent steps that depend on this dataset\n",
        "    dataset_lima = [] # Ensure it's initialized as empty if loading failed"
      ],
      "metadata": {
        "id": "Ks7mtSMpUfHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##17: Filter Single-Turn Examples"
      ],
      "metadata": {
        "id": "CUJAolwrUgzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for single-turn conversations only\n",
        "single_turn_outputs = []\n",
        "for example in dataset_lima:\n",
        "    conversations = example['conversations']\n",
        "    if len(conversations) == 2:  # Exactly 2 messages = single turn\n",
        "        output_text = conversations[1]  # Assistant's response\n",
        "        if len(output_text) < 1500:  # Reasonable length\n",
        "            single_turn_outputs.append(output_text)\n",
        "\n",
        "print(f\"‚úÖ Found {len(single_turn_outputs)} single-turn examples\")\n",
        "\n",
        "# Sample 150 as required\n",
        "sampled_outputs = random.sample(single_turn_outputs, min(CONFIG[\"lima_sample_size\"], len(single_turn_outputs)))\n",
        "print(f\"üìä Sampled {len(sampled_outputs)} outputs\")"
      ],
      "metadata": {
        "id": "pxpxICaXUrod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##18: Generate Instructions Function"
      ],
      "metadata": {
        "id": "NVaNLJWQUuIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_instruction_fixed(model, tokenizer_to_use, output_text, device):\n",
        "    \"\"\"Generate instruction for given output\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    if hasattr(model, 'gradient_checkpointing_disable'):\n",
        "        model.gradient_checkpointing_disable()\n",
        "\n",
        "    input_text = f\"### Output:\\n{output_text[:300]}\\n\\n### Instruction:\"\n",
        "\n",
        "    inputs = tokenizer_to_use(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=30,  # Reduced for speed\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer_to_use.pad_token_id,\n",
        "            eos_token_id=tokenizer_to_use.eos_token_id,\n",
        "            use_cache=True  # Explicitly enable cache\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer_to_use.decode(\n",
        "        generated_ids[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "zG9jWZjCU9MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19: Generate Instructions from LIMA\n",
        "\n",
        "Using the trained backward model, we now perform the instruction backtranslation step. We feed the sampled single-turn outputs from the LIMA dataset into the backward model to generate new, synthetic instructions. This creates the augmented instruction-output pairs."
      ],
      "metadata": {
        "id": "3DNO30kiVAAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nü§ñ Generating instructions...\")\n",
        "generated_instructions = []\n",
        "\n",
        "# Use appropriate model and tokenizer\n",
        "model_to_use = backward_model\n",
        "tokenizer_to_use = prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer\n",
        "\n",
        "# Put model in eval mode and disable gradient checkpointing\n",
        "model_to_use.eval()\n",
        "if hasattr(model_to_use, 'gradient_checkpointing_disable'):\n",
        "    model_to_use.gradient_checkpointing_disable()\n",
        "\n",
        "# Generate with progress bar\n",
        "for i in tqdm(range(len(sampled_outputs)), desc=\"Generating\"):\n",
        "    try:\n",
        "        instruction = generate_instruction_fixed(\n",
        "            model_to_use,\n",
        "            tokenizer_to_use,\n",
        "            sampled_outputs[i],\n",
        "            device\n",
        "        )\n",
        "        generated_instructions.append(instruction)\n",
        "\n",
        "        # Clear cache periodically\n",
        "        if device.type == \"cuda\" and i % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error at index {i}: {e}\")\n",
        "        generated_instructions.append(\"Error generating instruction\")\n",
        "\n",
        "print(f\"‚úÖ Generated {len(generated_instructions)} instructions\")"
      ],
      "metadata": {
        "id": "6rrpvGMmTgQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\nü§ñ Generating instructions...\")\n",
        "# generated_instructions = []\n",
        "\n",
        "# # Use appropriate model and tokenizer\n",
        "# model_to_use = backward_model\n",
        "# tokenizer_to_use = prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer\n",
        "\n",
        "# # Generate in small batches to show progress\n",
        "# for i in tqdm(range(0, len(sampled_outputs), 10), desc=\"Generating\"):\n",
        "#     batch = sampled_outputs[i:i+10]\n",
        "#     for output_text in batch:\n",
        "#         instruction = generate_instruction(model_to_use, tokenizer_to_use, output_text, device)\n",
        "#         generated_instructions.append(instruction)\n",
        "\n",
        "# print(f\"‚úÖ Generated {len(generated_instructions)} instructions\")"
      ],
      "metadata": {
        "id": "92v00I9cVE7Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20: Create Augmented Dataset"
      ],
      "metadata": {
        "id": "QbU7xxfjVFvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create augmented dataset\n",
        "augmented_data = []\n",
        "for instruction, output in zip(generated_instructions, sampled_outputs):\n",
        "    augmented_data.append({\n",
        "        'instruction': instruction,\n",
        "        'output': output\n",
        "    })\n",
        "\n",
        "# Save augmented data\n",
        "with open('./data/augmented_data.json', 'w') as f:\n",
        "    json.dump(augmented_data, f)"
      ],
      "metadata": {
        "id": "mBfALJRtVPS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21: Print 5 Generated Examples\n",
        "\n",
        "5 examples of the instruction-output pairs generated in the previous step. These pairs consist of a synthetic instruction generated by the backward model and the original LIMA output."
      ],
      "metadata": {
        "id": "KdoIveXyVQzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìã 5 EXAMPLES OF GENERATED INSTRUCTIONS :\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(min(5, len(augmented_data))):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    # Adjusted format based on user requirement\n",
        "    print(f\"üîπ Generated Instruction: {augmented_data[i]['instruction']}\")\n",
        "    print(f\"üîπ LIMA Output: {augmented_data[i]['output'][:200]}...\") # Truncating for brevity in display\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "M80eKFwcVTnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##22: Setup Rating Model"
      ],
      "metadata": {
        "id": "XwQVy-k0VVLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Self-Curation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clear memory if on GPU\n",
        "if device.type == \"cuda\":\n",
        "    del backward_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# For rating, we'll use the same model\n",
        "if CONFIG[\"use_cpu_prototype\"]:\n",
        "    print(\"üîß Using small model for rating prototype...\")\n",
        "    rating_model = model_to_use\n",
        "    rating_tokenizer = tokenizer_to_use\n",
        "else:\n",
        "    print(\"ü§ñ Loading Llama-2-7b-chat for rating...\")\n",
        "    rating_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    rating_tokenizer = tokenizer"
      ],
      "metadata": {
        "id": "ZLCUqF1cVY07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##23: Rating Prompt Template"
      ],
      "metadata": {
        "id": "hpl3nelvVajS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified rating prompt for efficiency\n",
        "rating_prompt_template = \"\"\"Rate the quality of this instruction-response pair on a scale of 1-5.\n",
        "\n",
        "1 = Very poor (off-topic, incomplete, or irrelevant)\n",
        "2 = Poor (partially addresses the question)\n",
        "3 = Fair (adequate but not from AI assistant perspective)\n",
        "4 = Good (clear, comprehensive, helpful)\n",
        "5 = Excellent (perfect AI assistant response)\n",
        "\n",
        "Here are some examples:\n",
        "\n",
        "Instruction: Tell me about the history of the internet.\n",
        "Response: The internet started as a project by the US Department of Defense called ARPANET in the late 1960s... (Full history)\n",
        "Reason: This response is comprehensive and directly answers the instruction.\n",
        "Score: 5\n",
        "\n",
        "Instruction: What is the best color?\n",
        "Response: I like pizza.\n",
        "Reason: This response is completely irrelevant to the instruction.\n",
        "Score: 1\n",
        "\n",
        "Instruction: {instruction}\n",
        "Response: {output}\n",
        "\n",
        "Provide a brief reason and then write \"Score: X\" where X is 1-5.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JT0mGDqAVfAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##24: Rating Function"
      ],
      "metadata": {
        "id": "2Qt37OgrVhMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_example(model, tokenizer_to_use, instruction, output, device):\n",
        "    \"\"\"Rate an instruction-output pair\"\"\"\n",
        "    prompt = rating_prompt_template.format(\n",
        "        instruction=instruction,\n",
        "        output=output[:500]  # Truncate long outputs\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer_to_use(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer_to_use.pad_token_id,\n",
        "            eos_token_id=tokenizer_to_use.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer_to_use.decode(\n",
        "        generated_ids[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Extract score\n",
        "    score_match = re.search(r\"Score:\\s*(\\d)\", generated_text)\n",
        "    if score_match:\n",
        "        score = int(score_match.group(1))\n",
        "        reasoning = generated_text.split(\"Score:\")[0].strip()\n",
        "    else:\n",
        "        score = 3  # Default\n",
        "        reasoning = \"Could not parse score\"\n",
        "\n",
        "    return score, reasoning"
      ],
      "metadata": {
        "id": "RPhHwscvVo1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##25: Score Examples\n",
        "\n",
        "In the self-curation step, we use a rating model (LLM) to evaluate the quality of the augmented instruction-output pairs generated from the LIMA data. This cell iterates through the augmented data, sends each pair to the rating model with a specific prompt, and records a quality score (1-5) and the reasoning provided by the rater model."
      ],
      "metadata": {
        "id": "gAm_277hVscY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìä Scoring augmented examples...\")\n",
        "scored_data = []\n",
        "high_quality = []\n",
        "low_quality = []\n",
        "\n",
        "# Score a subset for speed\n",
        "num_to_score = min(50, len(augmented_data)) if CONFIG[\"use_cpu_prototype\"] else len(augmented_data)\n",
        "\n",
        "for i in tqdm(range(num_to_score), desc=\"Scoring\"):\n",
        "    example = augmented_data[i]\n",
        "    score, reasoning = rate_example(\n",
        "        rating_model,\n",
        "        rating_tokenizer,\n",
        "        example['instruction'],\n",
        "        example['output'],\n",
        "        device\n",
        "    )\n",
        "\n",
        "    scored_example = {\n",
        "        'instruction': example['instruction'],\n",
        "        'output': example['output'],\n",
        "        'score': score,\n",
        "        'reasoning': reasoning\n",
        "    }\n",
        "    scored_data.append(scored_example)\n",
        "\n",
        "    # Collect high/low quality examples\n",
        "    if score >= 4 and len(high_quality) < 5:\n",
        "        high_quality.append(scored_example)\n",
        "    elif score <= 2 and len(low_quality) < 5:\n",
        "        low_quality.append(scored_example)"
      ],
      "metadata": {
        "id": "j5CT6_rWVxRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##26: Print High Quality Examples\n",
        "\n",
        "5 examples of the augmented instruction-output pairs that were rated as high quality (score >= 4) during the self-curation process. These examples represent the pairs that will be used to create the curated dataset for the final instruction tuning."
      ],
      "metadata": {
        "id": "qV4lNA-UVzlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ 5 HIGH QUALITY EXAMPLES (Score >= 4) :\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, ex in enumerate(high_quality[:5]):\n",
        "    print(f\"\\nHigh Quality Example {i+1}:\")\n",
        "    print(f\"üìä Score: {ex['score']}\")\n",
        "    print(f\"üîπ Instruction: {ex['instruction']}\")\n",
        "    print(f\"üîπ Output (first 200 chars): {ex['output'][:200]}...\")\n",
        "    print(f\"üí≠ Reasoning: {ex['reasoning'][:150]}...\")\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "t_aHN0pFV3P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##27: Print Low Quality Examples\n",
        "\n",
        "5 examples of the augmented instruction-output pairs that were rated as low quality (score <= 2) during the self-curation process. These examples illustrate the types of pairs that were filtered out and not included in the curated dataset."
      ],
      "metadata": {
        "id": "mLrGMKCiV4va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚ùå 5 LOW QUALITY EXAMPLES (Score <= 2) :\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, ex in enumerate(low_quality[:5]):\n",
        "    print(f\"\\nLow Quality Example {i+1}:\")\n",
        "    print(f\"üìä Score: {ex['score']}\")\n",
        "    print(f\"üîπ Instruction: {ex['instruction']}\")\n",
        "    print(f\"üîπ Output (first 200 chars): {ex['output'][:200]}...\")\n",
        "    print(f\"üí≠ Reasoning: {ex['reasoning'][:150]}...\")\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "voS07jnRV8BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##28: Create and Upload Curated Dataset\n",
        "\n",
        "This step filters the scored augmented data to create the final curated dataset, including only the high-quality examples (score >= 4). This dataset is then uploaded to the Hugging Face Hub, making it available for the final instruction tuning step. Both the curated and the full scored datasets are uploaded."
      ],
      "metadata": {
        "id": "9ppMtUKwWEYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter high quality examples (score >= 4)\n",
        "curated_data = [ex for ex in scored_data if ex['score'] >= 4]\n",
        "print(f\"\\n‚úÖ Curated dataset size: {len(curated_data)} high-quality examples\")\n",
        "\n",
        "# Create HuggingFace dataset\n",
        "curated_dataset = Dataset.from_list(curated_data)\n",
        "all_scored_dataset = Dataset.from_list(scored_data)\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    'curated': curated_dataset,\n",
        "    'all_scored': all_scored_dataset\n",
        "})\n",
        "\n",
        "if not CONFIG[\"use_cpu_prototype\"]:\n",
        "    # Upload to HuggingFace\n",
        "    dataset_repo_name = f\"{HF_USERNAME}/instruction-backtranslation-curated\"\n",
        "    dataset_dict.push_to_hub(dataset_repo_name, token=hf_token)\n",
        "    dataset_url = f\"https://huggingface.co/datasets/{dataset_repo_name}\"\n",
        "    print(f\"‚úÖ DATASET URL: {dataset_url}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CPU prototype mode - skipping dataset upload\")\n",
        "    dataset_url = \"CPU_PROTOTYPE_MODE\""
      ],
      "metadata": {
        "id": "IutdQZ-5WIui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##29: Prepare Final Training Data"
      ],
      "metadata": {
        "id": "b8wctT3VWJaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Fine-tune on Curated Dataset\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Clear memory\n",
        "if device.type == \"cuda\":\n",
        "    del rating_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Combine seed data with curated data\n",
        "combined_data = []\n",
        "\n",
        "# Add some original seed data\n",
        "seed_size = min(len(curated_data), len(all_pairs))\n",
        "for i in range(seed_size):\n",
        "    combined_data.append({\n",
        "        'instruction': all_pairs[i]['instruction'],\n",
        "        'output': all_pairs[i]['output']\n",
        "    })\n",
        "\n",
        "# Add curated augmented data\n",
        "for item in curated_data:\n",
        "    combined_data.append({\n",
        "        'instruction': item['instruction'],\n",
        "        'output': item['output']\n",
        "    })\n",
        "\n",
        "print(f\"üìä Combined dataset: {len(combined_data)} examples\")"
      ],
      "metadata": {
        "id": "6VSPU2cAWMJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##30: Create Instruction Dataset"
      ],
      "metadata": {
        "id": "tkeAtY1CWOhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format for instruction tuning\n",
        "instruction_texts = []\n",
        "for item in combined_data:\n",
        "    text = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n{item['output']}\"\n",
        "    instruction_texts.append(text)\n",
        "\n",
        "instruction_dataset = Dataset.from_dict({'text': instruction_texts})\n",
        "\n",
        "# Tokenize\n",
        "tokenized_instruction = instruction_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "tokenized_instruction = tokenized_instruction.map(\n",
        "    lambda examples: {\"labels\": examples[\"input_ids\"]},\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "aEcuOeoeWRh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##31: Setup Final Model"
      ],
      "metadata": {
        "id": "YmwWldkiWS48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CONFIG[\"use_cpu_prototype\"]:\n",
        "    print(\"üîß Using prototype model for final training...\")\n",
        "    instruction_model = AutoModelForCausalLM.from_pretrained(prototype_model_id)\n",
        "else:\n",
        "    print(\"ü§ñ Loading model for instruction tuning...\")\n",
        "    instruction_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    instruction_model = prepare_model_for_kbit_training(instruction_model)\n",
        "    instruction_model = get_peft_model(instruction_model, lora_config)\n",
        "    instruction_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "gV7MvE8ZWWZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##32: Train Final Model\n",
        "\n",
        "This is the final instruction tuning step. The base model is fine-tuned on the combined dataset, which includes both a portion of the original seed data and the newly curated high-quality augmented instruction-output pairs. This aims to improve the model's ability to follow instructions based on the self-augmented data."
      ],
      "metadata": {
        "id": "RStFlx2MWYfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüöÄ Training instruction-tuned model...\")\n",
        "\n",
        "trainer_instruction = Trainer(\n",
        "    model=instruction_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_instruction,\n",
        "    tokenizer=prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(\n",
        "        tokenizer=prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        ")\n",
        "\n",
        "trainer_instruction.train()\n",
        "print(\"‚úÖ Instruction tuning complete!\")"
      ],
      "metadata": {
        "id": "3nhj3w2dWd4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 33: Generate Example Responses\n",
        "\n",
        "As a required deliverable, this cell uses the newly trained instruction-tuned model to generate responses for a set of test prompts. This demonstrates the model's ability to follow instructions after the fine-tuning process."
      ],
      "metadata": {
        "id": "U9VotOmlWgP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ü§ñ 5 EXAMPLE RESPONSES FROM FINAL MODEL :\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_prompts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How do I make coffee?\",\n",
        "    \"Explain photosynthesis simply.\",\n",
        "    \"What are the benefits of exercise?\",\n",
        "    \"How does the internet work?\"\n",
        "]\n",
        "\n",
        "instruction_model.eval()\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"üìù Instruction: {prompt}\")\n",
        "\n",
        "    input_text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        "    inputs = (prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer)(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = instruction_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=(prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer).pad_token_id,\n",
        "            eos_token_id=(prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer).eos_token_id\n",
        "        )\n",
        "\n",
        "    response = (prototype_tokenizer if CONFIG[\"use_cpu_prototype\"] else tokenizer).decode(\n",
        "        outputs[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    response = response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    print(f\"ü§ñ Response: {response}\")\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "id": "iIa8J0fbWkJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 34: Upload Final Model\n",
        "\n",
        "After the instruction tuning is complete, the final fine-tuned model is saved and uploaded to the Hugging Face Hub. This is the final deliverable model resulting from the self-alignment process."
      ],
      "metadata": {
        "id": "SYA8Ha85WmcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not CONFIG[\"use_cpu_prototype\"]:\n",
        "    print(\"\\nüíæ Uploading final instruction-tuned model...\")\n",
        "    final_model_name = f\"{HF_USERNAME}/llama2-7b-instruction-tuned\"\n",
        "\n",
        "    instruction_model.push_to_hub(final_model_name, use_auth_token=True)\n",
        "    tokenizer.push_to_hub(final_model_name, use_auth_token=True)\n",
        "\n",
        "    final_model_url = f\"https://huggingface.co/{final_model_name}\"\n",
        "    print(f\"‚úÖ FINAL MODEL URL: {final_model_url}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CPU prototype mode - skipping final model upload\")\n",
        "    final_model_url = \"CPU_PROTOTYPE_MODE\""
      ],
      "metadata": {
        "id": "TTe_nuhmWpKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##35: Final Summary"
      ],
      "metadata": {
        "id": "LCREa5UOWqu7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asgGF2rlE5E5"
      },
      "outputs": [],
      "source": [
        "print(\" Self-Alignment with Instruction Backtranslation\")\n",
        "print(\"\\nüìä DELIVERABLES SUMMARY:\")\n",
        "print(f\"1. Backward Model URL: {backward_model_url}\")\n",
        "print(f\"2. Generated 5 instruction examples ‚úÖ\")\n",
        "print(f\"3. Showed 5 high-quality examples ‚úÖ\")\n",
        "print(f\"4. Showed 5 low-quality examples ‚úÖ\")\n",
        "print(f\"5. Curated Dataset URL: {dataset_url}\")\n",
        "print(f\"6. Final Model URL: {final_model_url}\")\n",
        "print(f\"7. Generated 5 example responses ‚úÖ\")\n",
        "\n",
        "if CONFIG[\"use_cpu_prototype\"]:\n",
        "    print(\"\\n‚ö†Ô∏è NOTE: Running in CPU prototype mode\")\n",
        "    print(\"To run full version on GPU:\")\n",
        "    print(\"1. Set CONFIG['use_cpu_prototype'] = False\")\n",
        "    print(\"2. Ensure GPU runtime is selected\")\n",
        "    print(\"3. Re-run all cells\")\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    \"backward_model_url\": backward_model_url,\n",
        "    \"dataset_url\": dataset_url,\n",
        "    \"final_model_url\": final_model_url,\n",
        "    \"mode\": \"CPU_PROTOTYPE\" if CONFIG[\"use_cpu_prototype\"] else \"FULL_GPU\"\n",
        "}\n",
        "\n",
        "with open(\"assignment3_summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Summary saved to assignment3_summary.json\")"
      ]
    }
  ]
}